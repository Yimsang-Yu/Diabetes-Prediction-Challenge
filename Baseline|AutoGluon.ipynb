{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39668d6e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-28T18:50:36.910896Z",
     "iopub.status.busy": "2025-12-28T18:50:36.910474Z",
     "iopub.status.idle": "2025-12-28T20:03:18.136987Z",
     "shell.execute_reply": "2025-12-28T20:03:18.135770Z"
    },
    "papermill": {
     "duration": 4361.243335,
     "end_time": "2025-12-28T20:03:18.149258",
     "exception": false,
     "start_time": "2025-12-28T18:50:36.905923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.9/98.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\r\n",
      "datasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 20.0.0 which is incompatible.\r\n",
      "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 20.0.0 which is incompatible.\r\n",
      "bigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\r\n",
      "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 20.0.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mtrain_path: /kaggle/input/playground-series-s5e12/train.csv\n",
      "test_path : /kaggle/input/playground-series-s5e12/test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.5.0\n",
      "Python Version:     3.12.12\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Sat Sep 27 10:16:09 UTC 2025\n",
      "CPU Count:          4\n",
      "Pytorch Version:    2.8.0+cu126\n",
      "CUDA Version:       CUDA is not available\n",
      "Memory Avail:       29.60 GB / 31.35 GB (94.4%)\n",
      "Disk Space Avail:   19.50 GB / 19.52 GB (99.9%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=5, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 900s of the 3600s of remaining time (25%).\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2025-12-28 18:51:22,237\tINFO worker.py:2023 -- Started a local Ray instance.\n",
      "/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py:2062: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "\t\tContext path: \"/kaggle/working/ag_diabetes_autml/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m Beginning AutoGluon training ... Time limit = 881s\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m AutoGluon will save models to \"/kaggle/working/ag_diabetes_autml/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m Train Data Rows:    622222\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m Train Data Columns: 24\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m Label Column:       diagnosed_diabetes\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m Problem Type:       binary\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \tAvailable Memory:                    29455.80 MB\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \tTrain Data (Original)  Memory Usage: 76.55 MB (0.3% of available memory)\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t\t('category', []) :  9 | ['gender', 'ethnicity', 'education_level', 'income_level', 'smoking_status', ...]\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t\t('float', [])    :  5 | ['diet_score', 'sleep_hours_per_day', 'screen_time_hours_per_day', 'bmi', 'waist_to_hip_ratio']\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t\t('int', [])      : 10 | ['age', 'alcohol_consumption_per_week', 'physical_activity_minutes_per_week', 'systolic_bp', 'diastolic_bp', ...]\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t\t('category', [])  :  6 | ['gender', 'ethnicity', 'education_level', 'income_level', 'smoking_status', ...]\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t\t('float', [])     :  5 | ['diet_score', 'sleep_hours_per_day', 'screen_time_hours_per_day', 'bmi', 'waist_to_hip_ratio']\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t\t('int', [])       : 10 | ['age', 'alcohol_consumption_per_week', 'physical_activity_minutes_per_week', 'systolic_bp', 'diastolic_bp', ...]\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t\t('int', ['bool']) :  3 | ['family_history_diabetes', 'hypertension_history', 'cardiovascular_history']\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t1.6s = Fit runtime\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t24 features in original data used to generate 24 features in processed data.\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \tTrain Data (Processed) Memory Usage: 76.55 MB (0.3% of available memory)\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m Data preprocessing and feature engineering runtime = 1.8s ...\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 586.06s of the 879.30s of remaining time.\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=1.90%)\n",
      "\u001b[36m(pid=gcs_server)\u001b[0m [2025-12-28 18:51:46,153 E 60 60] (gcs_server) gcs_server.cc:303: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[36m(_ray_fit pid=466)\u001b[0m /usr/local/lib/python3.12/dist-packages/sqlalchemy/orm/query.py:195: SyntaxWarning: \"is not\" with 'tuple' literal. Did you mean \"!=\"?\n",
      "\u001b[36m(_ray_fit pid=466)\u001b[0m   if entities is not ():\n",
      "\u001b[33m(raylet)\u001b[0m [2025-12-28 18:51:51,341 E 135 135] (raylet) main.cc:979: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m [2025-12-28 18:51:58,687 E 176 258] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[36m(_ray_fit pid=463)\u001b[0m /usr/local/lib/python3.12/dist-packages/sqlalchemy/orm/query.py:195: SyntaxWarning: \"is not\" with 'tuple' literal. Did you mean \"!=\"?\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=463)\u001b[0m   if entities is not ():\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "[2025-12-28 18:51:59,164 E 17 173] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[36m(_ray_fit pid=466)\u001b[0m [2025-12-28 18:52:11,436 E 466 501] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[36m(_ray_fit pid=464)\u001b[0m [2025-12-28 18:52:11,454 E 464 510] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=463)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.595547\n",
      "\u001b[36m(_ray_fit pid=465)\u001b[0m [2000]\tvalid_set's binary_logloss: 0.597063\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=466)\u001b[0m \tRan out of time, early stopping on iteration 2199. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=466)\u001b[0m \t[2199]\tvalid_set's binary_logloss: 0.594616\n",
      "\u001b[36m(_ray_fit pid=463)\u001b[0m [2025-12-28 18:52:11,527 E 463 566] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=463)\u001b[0m \tRan out of time, early stopping on iteration 2201. Best iteration is:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=463)\u001b[0m \t[2201]\tvalid_set's binary_logloss: 0.594443\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=593)\u001b[0m [2025-12-28 18:57:20,025 E 593 616] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=593)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.597954\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=593)\u001b[0m [2000]\tvalid_set's binary_logloss: 0.597245\n",
      "\u001b[36m(_ray_fit pid=593)\u001b[0m [3000]\tvalid_set's binary_logloss: 0.596723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=593)\u001b[0m \tRan out of time, early stopping on iteration 3828. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=593)\u001b[0m \t[3694]\tvalid_set's binary_logloss: 0.596514\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t0.707\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t544.51s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t275.35s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 271.30s of remaining time.\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \tFitting 1 model on all data | Fitting with cpus=4, gpus=0, mem=0.0/27.9 GB\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t0.707\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t0.2s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t0.17s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m Fitting 108 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 270.86s of the 270.77s of remaining time.\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=1.97%)\n",
      "\u001b[36m(_ray_fit pid=703)\u001b[0m [2025-12-28 19:02:17,540 E 703 750] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[36m(_ray_fit pid=831)\u001b[0m [2025-12-28 19:03:09,094 E 831 851] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t0.7069\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t83.58s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t16.39s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 177.41s of the 177.32s of remaining time.\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=1.99%)\n",
      "\u001b[36m(_ray_fit pid=939)\u001b[0m [2025-12-28 19:03:51,364 E 939 979] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[36m(_ray_fit pid=939)\u001b[0m \tRan out of time, early stopping on iteration 692. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=939)\u001b[0m \t[680]\tvalid_set's binary_logloss: 0.584807\n",
      "\u001b[36m(_ray_fit pid=941)\u001b[0m [2025-12-28 19:03:51,493 E 941 1044] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=942)\u001b[0m \tRan out of time, early stopping on iteration 579. Best iteration is:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=942)\u001b[0m \t[574]\tvalid_set's binary_logloss: 0.584914\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1071)\u001b[0m [2025-12-28 19:05:26,131 E 1071 1094] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t0.7247\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t153.43s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t56.52s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 9.94s of the 9.85s of remaining time.\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=4, gpus=0, mem=2.9/27.8 GB\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \tWarning: Model is expected to require 341.9s to train, which exceeds the maximum time limit of 9.5s, skipping model...\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \tTime limit exceeded... Skipping RandomForestGini_BAG_L2.\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 4.46s of remaining time.\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \tFitting 1 model on all data | Fitting with cpus=4, gpus=0, mem=0.0/27.9 GB\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \tEnsemble Weights: {'LightGBM_BAG_L2': 1.0}\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t0.7247\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t11.78s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m \t0.16s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m AutoGluon training complete, total runtime = 888.72s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 374.9 rows/s (124445 batch size)\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/ag_diabetes_autml/ds_sub_fit/sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=176)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                 model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      LightGBM_BAG_L2       0.726371   0.724664     roc_auc       72.762575     331.869661  697.937434                 9.404634               56.515401         153.428516            2       True          4\n",
      "1  WeightedEnsemble_L3       0.726371   0.724664     roc_auc       72.765712     332.027755  709.713343                 0.003137                0.158094          11.775908            3       True          5\n",
      "2    LightGBMXT_BAG_L2       0.708780   0.706865     roc_auc       68.259452     291.746919  628.090654                 4.901511               16.392658          83.581735            2       True          3\n",
      "3    LightGBMXT_BAG_L1       0.708500   0.707050     roc_auc       63.357941     275.354260  544.508919                63.357941              275.354260         544.508919            1       True          1\n",
      "4  WeightedEnsemble_L2       0.708500   0.707050     roc_auc       63.361864     275.527777  544.704953                 0.003923                0.173517           0.196035            2       True          2\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t990s\t = DyStack   runtime |\t2610s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 2610s\n",
      "AutoGluon will save models to \"/kaggle/working/ag_diabetes_autml\"\n",
      "Train Data Rows:    700000\n",
      "Train Data Columns: 24\n",
      "Label Column:       diagnosed_diabetes\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    28698.54 MB\n",
      "\tTrain Data (Original)  Memory Usage: 86.12 MB (0.3% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  9 | ['gender', 'ethnicity', 'education_level', 'income_level', 'smoking_status', ...]\n",
      "\t\t('float', [])    :  5 | ['diet_score', 'sleep_hours_per_day', 'screen_time_hours_per_day', 'bmi', 'waist_to_hip_ratio']\n",
      "\t\t('int', [])      : 10 | ['age', 'alcohol_consumption_per_week', 'physical_activity_minutes_per_week', 'systolic_bp', 'diastolic_bp', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :  6 | ['gender', 'ethnicity', 'education_level', 'income_level', 'smoking_status', ...]\n",
      "\t\t('float', [])     :  5 | ['diet_score', 'sleep_hours_per_day', 'screen_time_hours_per_day', 'bmi', 'waist_to_hip_ratio']\n",
      "\t\t('int', [])       : 10 | ['age', 'alcohol_consumption_per_week', 'physical_activity_minutes_per_week', 'systolic_bp', 'diastolic_bp', ...]\n",
      "\t\t('int', ['bool']) :  3 | ['family_history_diabetes', 'hypertension_history', 'cardiovascular_history']\n",
      "\t1.8s = Fit runtime\n",
      "\t24 features in original data used to generate 24 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 86.12 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 2.03s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1738.15s of the 2607.87s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=1.98%)\n",
      "\t0.7098\t = Validation score   (roc_auc)\n",
      "\t1561.19s\t = Training   runtime\n",
      "\t1478.78s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 8.27s of the 877.98s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=2.01%)\n",
      "\t0.6769\t = Validation score   (roc_auc)\n",
      "\t20.65s\t = Training   runtime\n",
      "\t0.61s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 848.95s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=4, gpus=0, mem=0.0/28.2 GB\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.875, 'LightGBM_BAG_L1': 0.125}\n",
      "\t0.7098\t = Validation score   (roc_auc)\n",
      "\t8.72s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting 108 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 840.01s of the 839.96s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=2.10%)\n",
      "\t0.7099\t = Validation score   (roc_auc)\n",
      "\t94.66s\t = Training   runtime\n",
      "\t18.63s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 733.82s of the 733.77s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=2.12%)\n",
      "\t0.7249\t = Validation score   (roc_auc)\n",
      "\t352.41s\t = Training   runtime\n",
      "\t114.21s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 353.47s of the 353.42s of remaining time.\n",
      "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=4, gpus=0, mem=3.3/28.2 GB\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 255 due to low time. Expected time usage reduced from 414.8s -> 353.0s...\n",
      "\tNot enough time to generate out-of-fold predictions for model. Estimated time required was 211.54s compared to 117.01s of available time.\n",
      "\tTime limit exceeded... Skipping RandomForestGini_BAG_L2.\n",
      "Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 10.48s of the 10.42s of remaining time.\n",
      "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=4, gpus=0, mem=3.3/27.7 GB\n",
      "\tWarning: Model is expected to require 410.2s to train, which exceeds the maximum time limit of 10.0s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestEntr_BAG_L2.\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 3.97s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=4, gpus=0, mem=0.0/28.1 GB\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L2': 1.0}\n",
      "\t0.7249\t = Validation score   (roc_auc)\n",
      "\t17.45s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2623.72s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 87.8 rows/s (140000 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/ag_diabetes_autml\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ saved submission.csv\n",
      "       id  diagnosed_diabetes\n",
      "0  700000            0.483897\n",
      "1  700001            0.720819\n",
      "2  700002            0.765071\n",
      "3  700003            0.391099\n",
      "4  700004            0.950468\n",
      "\n",
      "=== AutoGluon Leaderboard (top) ===\n",
      "                 model  score_val eval_metric  pred_time_val     fit_time  \\\n",
      "0      LightGBM_BAG_L2   0.724857     roc_auc    1593.596610  1934.247614   \n",
      "1  WeightedEnsemble_L3   0.724857     roc_auc    1593.767296  1951.695249   \n",
      "2    LightGBMXT_BAG_L2   0.709858     roc_auc    1498.018157  1676.500010   \n",
      "3  WeightedEnsemble_L2   0.709831     roc_auc    1479.563917  1590.555925   \n",
      "4    LightGBMXT_BAG_L1   0.709816     roc_auc    1478.778527  1561.190172   \n",
      "5      LightGBM_BAG_L1   0.676950     roc_auc       0.612761    20.649909   \n",
      "\n",
      "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
      "0              114.205322         352.407533            2       True   \n",
      "1                0.170686          17.447635            3       True   \n",
      "2               18.626870          94.659929            2       True   \n",
      "3                0.172630           8.715843            2       True   \n",
      "4             1478.778527        1561.190172            1       True   \n",
      "5                0.612761          20.649909            1       True   \n",
      "\n",
      "   fit_order  \n",
      "0          5  \n",
      "1          6  \n",
      "2          4  \n",
      "3          3  \n",
      "4          1  \n",
      "5          2  \n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# #AutoML #AutoGluon #TabularData #BinaryClassification\n",
    "# #Kaggle #Ensembling #Stacking #ModelSelection #MachineLearning\n",
    "# ============================================================\n",
    "\n",
    "# If running on Kaggle: you can keep this install line\n",
    "!pip -q install -U \"autogluon.tabular>=1.1.0\"\n",
    "\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# -------------------------\n",
    "# Columns (as you specified)\n",
    "# -------------------------\n",
    "ID_COL = \"id\"\n",
    "TARGET = \"diagnosed_diabetes\"\n",
    "\n",
    "FEATURES = [\n",
    "    \"age\",\n",
    "    \"alcohol_consumption_per_week\",\n",
    "    \"physical_activity_minutes_per_week\",\n",
    "    \"diet_score\",\n",
    "    \"sleep_hours_per_day\",\n",
    "    \"screen_time_hours_per_day\",\n",
    "    \"bmi\",\n",
    "    \"waist_to_hip_ratio\",\n",
    "    \"systolic_bp\",\n",
    "    \"diastolic_bp\",\n",
    "    \"heart_rate\",\n",
    "    \"cholesterol_total\",\n",
    "    \"hdl_cholesterol\",\n",
    "    \"ldl_cholesterol\",\n",
    "    \"triglycerides\",\n",
    "    \"gender\",\n",
    "    \"ethnicity\",\n",
    "    \"education_level\",\n",
    "    \"income_level\",\n",
    "    \"smoking_status\",\n",
    "    \"employment_status\",\n",
    "    \"family_history_diabetes\",\n",
    "    \"hypertension_history\",\n",
    "    \"cardiovascular_history\",\n",
    "]\n",
    "\n",
    "CATEGORICAL = [\n",
    "    \"gender\",\n",
    "    \"ethnicity\",\n",
    "    \"education_level\",\n",
    "    \"income_level\",\n",
    "    \"smoking_status\",\n",
    "    \"employment_status\",\n",
    "    \"family_history_diabetes\",\n",
    "    \"hypertension_history\",\n",
    "    \"cardiovascular_history\",\n",
    "]\n",
    "NUMERIC = [c for c in FEATURES if c not in CATEGORICAL]\n",
    "\n",
    "# -------------------------\n",
    "# Helper: auto-find train/test\n",
    "# -------------------------\n",
    "def find_csv_by_name(root=\"/kaggle/input\", name=\"train.csv\"):\n",
    "    hits = glob.glob(os.path.join(root, \"**\", name), recursive=True)\n",
    "    return hits[0] if hits else None\n",
    "\n",
    "train_path = find_csv_by_name(name=\"train.csv\")\n",
    "test_path  = find_csv_by_name(name=\"test.csv\")\n",
    "\n",
    "# If your dataset uses different filenames, set them manually:\n",
    "train_path = \"/kaggle/input/playground-series-s5e12/train.csv\"\n",
    "test_path  = \"/kaggle/input/playground-series-s5e12/test.csv\"\n",
    "\n",
    "print(\"train_path:\", train_path)\n",
    "print(\"test_path :\", test_path)\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "test  = pd.read_csv(test_path)\n",
    "\n",
    "# -------------------------\n",
    "# Basic sanity checks\n",
    "# -------------------------\n",
    "needed_train = [ID_COL, TARGET] + FEATURES\n",
    "missing_train = [c for c in needed_train if c not in train.columns]\n",
    "if missing_train:\n",
    "    raise ValueError(f\"Missing columns in train: {missing_train}\")\n",
    "\n",
    "needed_test = [ID_COL] + FEATURES\n",
    "missing_test = [c for c in needed_test if c not in test.columns]\n",
    "if missing_test:\n",
    "    raise ValueError(f\"Missing columns in test: {missing_test}\")\n",
    "\n",
    "# -------------------------\n",
    "# Type casting (helps AutoML)\n",
    "# -------------------------\n",
    "def cast_types(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # categoricals to \"category\"\n",
    "    for c in CATEGORICAL:\n",
    "        df[c] = df[c].astype(\"category\")\n",
    "\n",
    "    # numerics to float (coerce errors -> NaN)\n",
    "    for c in NUMERIC:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "train = cast_types(train)\n",
    "test  = cast_types(test)\n",
    "\n",
    "# Ensure target is 0/1 int (if it isn't already)\n",
    "train[TARGET] = pd.to_numeric(train[TARGET], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# Use only required columns (prevents leakage from extra cols)\n",
    "train_ml = train[[ID_COL, TARGET] + FEATURES].copy()\n",
    "test_ml  = test[[ID_COL] + FEATURES].copy()\n",
    "\n",
    "# -------------------------\n",
    "# AutoGluon Tabular AutoML\n",
    "# -------------------------\n",
    "# Notes:\n",
    "# - If metric for the competition is unknown, ROC AUC is a solid default.\n",
    "# - If leaderboard uses logloss, you can switch eval_metric=\"log_loss\".\n",
    "predictor = TabularPredictor(\n",
    "    label=TARGET,\n",
    "    eval_metric=\"roc_auc\",\n",
    "    path=\"ag_diabetes_autml\",\n",
    "    verbosity=2\n",
    ")\n",
    "\n",
    "predictor.fit(\n",
    "    train_data=train_ml.drop(columns=[ID_COL]),\n",
    "    presets=\"best_quality\",   # strong ensemble (bagging + stacking), slower but usually best\n",
    "    time_limit=60*60,         # 1 hour; adjust as needed\n",
    "    num_bag_folds=5,\n",
    "    num_stack_levels=1\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Predict probabilities\n",
    "# -------------------------\n",
    "proba = predictor.predict_proba(test_ml.drop(columns=[ID_COL]))\n",
    "\n",
    "# AutoGluon returns a DataFrame for binary classification with 2 columns (class labels).\n",
    "# We want probability of class \"1\" (positive).\n",
    "if isinstance(proba, pd.DataFrame):\n",
    "    # pick column 1 if it exists, else take the \"largest\" label column\n",
    "    if 1 in proba.columns:\n",
    "        pred = proba[1].to_numpy()\n",
    "    else:\n",
    "        # fallback: choose the column that corresponds to positive class\n",
    "        # (often '1', 'True', or the max label)\n",
    "        col = sorted(proba.columns)[-1]\n",
    "        pred = proba[col].to_numpy()\n",
    "else:\n",
    "    # fallback if returned as series/array\n",
    "    pred = np.asarray(proba)\n",
    "\n",
    "# -------------------------\n",
    "# Save submission\n",
    "# -------------------------\n",
    "sub = pd.DataFrame({ID_COL: test_ml[ID_COL].values, TARGET: pred})\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"✅ saved submission.csv\")\n",
    "print(sub.head())\n",
    "\n",
    "# Optional: show leaderboard of models\n",
    "lb = predictor.leaderboard(silent=True)\n",
    "print(\"\\n=== AutoGluon Leaderboard (top) ===\")\n",
    "print(lb.head(15))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14272474,
     "sourceId": 91723,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4370.62285,
   "end_time": "2025-12-28T20:03:23.383407",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-28T18:50:32.760557",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
